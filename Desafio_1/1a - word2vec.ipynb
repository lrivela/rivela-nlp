{"cells":[{"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Word2vect\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kCED1hh-Ioyf"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PUbfVnzIIoMj"},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RIO7b8GjAC17"},"outputs":[],"source":["corpus = np.array(['que dia es hoy', 'martes el dia de hoy es martes', 'martes muchas gracias'])"]},{"cell_type":"markdown","metadata":{"id":"8WqdaTmO8P1r"},"source":["Documento 1 --> que dia es hoy \\\n","Documento 2 --> martes el dia de hoy es martes \\\n","Documento 3 --> martes muchas gracias"]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 1 - Obtener el vocabulario del corpus (los términos utilizados)\n","- Cada documento transformarlo en una lista de términos\n","- Armar un vector de términos no repetidos de todos los documentos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3ZqTOZzDI7uv"},"outputs":[{"data":{"text/plain":["array(['de', 'dia', 'el', 'es', 'gracias', 'hoy', 'martes', 'muchas',\n","       'que'], dtype='<U7')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["a = corpus[0].split(\" \")\n","b = corpus[1].split(\" \")\n","c = corpus[2].split(\" \")\n","unicos = np.sort(np.unique(np.array(a + b + c)))\n","unicos"]},{"cell_type":"markdown","metadata":{"id":"RUhH983FI7It"},"source":["### 2- OneHot encoding\n","Data una lista de textos, devolver una matriz con la representación oneHotEncoding de estos"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["indices = np.zeros( (3, len(unicos)) )"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Os0AAQo6I6Z1"},"outputs":[],"source":["indices_a = np.intersect1d(unicos, a, return_indices=True)[1]\n","indices_b = np.intersect1d(unicos, b, return_indices=True)[1]\n","indices_c = np.intersect1d(unicos, c, return_indices=True)[1]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["indices[0,indices_a] = 1\n","indices[1, indices_b] = 1\n","indices[2, indices_c] = 1"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0., 1., 0., 1., 0., 1., 0., 0., 1.],\n","       [1., 1., 1., 1., 0., 1., 1., 0., 0.],\n","       [0., 0., 0., 0., 1., 0., 1., 1., 0.]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["indices"]},{"cell_type":"markdown","metadata":{"id":"IIyWGmCpJVQL"},"source":["### 3- Vectores de frecuencia\n","Data una lista de textos, devolver una matriz con la representación de frecuencia de estos"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"yqij_7eHJbUi"},"outputs":[],"source":["frequency = np.copy(indices)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["a0 = np.unique(a, return_index=True, return_counts=True)[0]\n","a2 = np.unique(a, return_index=True, return_counts=True)[2]\n","repetidas_a = a0[np.argwhere(a2>1)]\n","indice_repetidas_a = np.intersect1d(unicos, a0[np.argwhere(a2>1)], return_indices=True)[1]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["b0 = np.unique(b, return_index=True, return_counts=True)[0]\n","b2 = np.unique(b, return_index=True, return_counts=True)[2]\n","repetidas_b = b0[np.argwhere(b2>1)]\n","indice_repetidas_b = np.intersect1d(unicos, b0[np.argwhere(b2>1)], return_indices=True)[1]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["c0 = np.unique(c, return_index=True, return_counts=True)[0]\n","c2 = np.unique(c, return_index=True, return_counts=True)[2]\n","repetidas_c = c0[np.argwhere(c2>1)]\n","indice_repetidas_c = np.intersect1d(unicos, c0[np.argwhere(c2>1)], return_indices=True)[1]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["frequency[0, indice_repetidas_a] = frequency[0, indice_repetidas_a] * a2[np.argwhere(a2>1)]\n","frequency[1, indice_repetidas_b] = frequency[1, indice_repetidas_b] * b2[np.argwhere(b2>1)]\n","frequency[2, indice_repetidas_c] = frequency[2, indice_repetidas_c] * c2[np.argwhere(c2>1)]"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0., 1., 0., 1., 0., 1., 0., 0., 1.],\n","       [1., 1., 1., 1., 0., 1., 2., 0., 0.],\n","       [0., 0., 0., 0., 1., 0., 1., 1., 0.]])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["frequency"]},{"cell_type":"markdown","metadata":{"id":"z_Ot8HvWJcBu"},"source":["### 4- TF-IDF\n","Data una lista de textos, devolver una matriz con la representacion TFIDF"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"waG_oWtpJjRw"},"outputs":[{"data":{"text/plain":["array([1., 2., 1., 2., 1., 2., 2., 1., 1.])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["idf = np.sum(indices, axis=0)\n","idf"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["array([3, 3, 3, 3, 3, 3, 3, 3, 3])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["idf_aux = np.full( len(unicos), 3 )\n","idf_aux"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["array([0.47712125, 0.17609126, 0.47712125, 0.17609126, 0.47712125,\n","       0.17609126, 0.17609126, 0.47712125, 0.47712125])"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["idf_final = idf_aux / idf\n","idf_final = np.log10(idf_final)\n","idf_final"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.         0.17609126 0.         0.17609126 0.         0.17609126\n","  0.         0.         0.47712125]\n"," [0.47712125 0.17609126 0.47712125 0.17609126 0.         0.17609126\n","  0.35218252 0.         0.        ]\n"," [0.         0.         0.         0.         0.47712125 0.\n","  0.17609126 0.47712125 0.        ]]\n"]}],"source":["tf_idf = frequency * idf_final\n","print(tf_idf)"]},{"cell_type":"markdown","metadata":{"id":"xMcsfndWJjm_"},"source":["### 5 - Comparación de documentos\n","Realizar una funcion que reciba el corpus y el índice de un documento y devuelva los documentos ordenados por la similitud coseno"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"CZdiop6IJpZN"},"outputs":[],"source":["## Asumo que la funcion recibe el corpus del comienzo y el indice es el documento al cual se le quiere evaluar la similitud con el resto de documentos."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def document_comparison(corpus, doc_idx):\n","    documents_list = []\n","    indices_list = []\n","    doc_count_list = []\n","    indices_repetidas_list = []\n","    result_list = []\n","\n","    for doc in corpus:\n","        lista_palabras = doc.split(\" \")\n","        documents_list.append(lista_palabras)\n","\n","    unicos = np.sort(np.unique(np.concatenate(documents_list)))\n","    # print(unicos)\n","\n","    ohe = np.zeros( (len(documents_list), len(unicos)) )\n","\n","    for doc in documents_list:\n","        indices_list.append( np.intersect1d(unicos, doc, return_indices=True)[1] )\n","\n","    for i in range(0, len(corpus) ):\n","        ohe[i, indices_list[i]] = 1\n","\n","    # print(ohe)\n","\n","    frequency = np.copy(ohe)\n","\n","    for doc in documents_list:\n","        doc_unique = np.unique(doc, return_index=True, return_counts=True)[0]\n","        doc_count = np.unique(doc, return_index=True, return_counts=True)[2]\n","        doc_count_list.append(doc_count)\n","        doc_repetidas = doc_unique[np.argwhere(doc_count>1)]\n","        doc_indice_repetidas = np.intersect1d(unicos, doc_unique[np.argwhere(doc_count>1)], return_indices=True)[1]\n","        \n","        indices_repetidas_list.append(doc_indice_repetidas)\n","\n","    for i in range(0, len(corpus) ):\n","        frequency[i, indices_repetidas_list[i]] = frequency[i, indices_repetidas_list[i]] * doc_count_list[i][np.argwhere(doc_count_list[i]>1)]\n","\n","    # print(frequency)\n","\n","    \n","    idf = np.sum(ohe, axis=0)\n","    idf_aux = np.full( len(unicos), len(documents_list) )\n","    idf_final = idf_aux / idf\n","    idf_final = np.log10(idf_final)\n","    tf_idf = frequency * idf_final\n","\n","    # print(tf_idf)\n","\n","    for i in range(0, len(corpus) ):\n","        result_list.append(cosine_similarity(tf_idf[doc_idx], tf_idf[i]))\n","\n","    return result_list\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["[0.9999999999999998, 0.20034190268098703, 0.0]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["document_comparison(corpus, 0)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5fRYTpympAwJSVbric6dW","collapsed_sections":[],"name":"1a - word2vec.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
